---
layout: post
title: "Machine Learning Quântico"
subtitle: Como funciona o QML?
date: 2025-12-16
author: Gabriel A. Belo
categories: [Computação, IA]
tags: [Computação Quântica | ML Quântico]
---

Esse conteúdo foi produzido durante o projeto de finalização da disciplina de Computação Quântica, no qual meu grupo ficou responsável por implementar um modelo de classificação utilizando Machine Learning Quântico (QML).

A minha parte do [projeto](https://github.com/SapoSopa/QML) foi fazer as explicações conceituais e a motivação a respeito do que foi utilizado na construção do Modelo Variacional Quântico (VQC), o qual compartilho agora neste post.

### Sumário

- [Motivação](#por-que-usar-quantum-machine-learning)
- [XOR](#dataset-xor--problema-não-linear)
- [Embedding](#embedding-de-dados-clássicos)
- [Modelos Variacionais](#o-que-são-modelos-variacionais)
- [Parâmetros Ajustáveis](#parâmetros-ajustáveis--nos-circuitos-parametrizados)
- [Gradientes](#quantum-gradients-parameter-shift-rule)

---

### Por que usar Quantum Machine Learning?

O Aprendizado de Máquina Quântico tem como objetivo explorar os princípios da mecânica quântica (superposição, emaranhamento, interferência...) para construir algoritmos que sejam mais poderosos do que os algoritmos clássicos de ML, buscando resolver problemas intratáveis ou com tempo de resolução muito maior em computadores clássicos.

A premissa é: se existe ganho em outras áreas ao comparar o quântico com o clássico, por que não haveria ganho também em Machine Learning?

Atualmente não existem provas universais de que algoritmos quânticos superem os algoritmos clássicos mais avançados (como Deep Learning), dada a natureza essencialmente empírica do campo de aprendizado de máquina. No entanto, já existem indícios teóricos que apontam para superioridade em certos cenários; além disso, ao tratar diretamente dados quânticos, é plausível que modelos quânticos tenham vantagem sobre a conversão para dados clássicos seguida de processamento clássico.

#### Problemas não lineares e classificação complexa

Algoritmos clássicos lineares têm dificuldade em separar dados que não são linearmente separáveis. Por outro lado, algoritmos quânticos possuem abordagens que lidam diretamente com essa não linearidade dos dados complexos.

Enquanto o ML clássico usa técnicas de kernel para mapear dados de baixa dimensão para um espaço de alta dimensão, onde um classificador linear pode atuar, o ML quântico faz esse mapeamento de forma natural através dos Quantum Embeddings (a codificação de dados clássicos em estados quânticos), e espera-se que o faça de maneira mais eficiente.

#### Circuitos quânticos parametrizados

O ML quântico utiliza esse tipo de circuito, também chamado de ansatz, como o núcleo do modelo variacional de aprendizado.

Vantagem:
- Essa estrutura permite a construção de algoritmos híbridos clássico-quânticos (adequados ao nível dos dispositivos quânticos atuais), em que a parte quântica lida com computação em altas dimensões e avaliação de expectativas, enquanto a parte clássica cuida da otimização dos parâmetros.

QNode: uma abstração que recebe uma função quântica (o circuito que realiza operações em qubits) e consegue executar e avaliar esse circuito em um device — o elo entre o circuito e o hardware quântico.

#### Espaço de produto tensorial

Considerado uma das maiores promessas do ML quântico, esse é o espaço no qual os vetores de estado de um sistema quântico residem, e ele possui características que podem permitir a computação de problemas muito complexos para a computação clássica. Entre as características:

- Crescimento exponencial: para n qubits, o espaço de Hilbert tem dimensão 2^n.
- Capacidade de representação: essa alta dimensionalidade pode ser utilizada para mapear dados em um espaço de features muito grande e representar modelos mais complexos que os clássicos.
- Aceleração: o processamento dessas altas dimensões pode, em alguns casos, ser tratável por técnicas clássicas; contudo espera-se que o ML quântico ofereça ganhos em eficiência para tarefas como classificação e clustering.

---

### Dataset: XOR — problema não linear

Dataset didático para ilustrar a necessidade de modelos não lineares.

No plano 2D, os pontos (0, 0), (0, 1), (1, 0) e (1, 1) com suas respectivas saídas não podem ser separados por uma única linha reta.

O ML quântico, através do mapeamento de dados no espaço de Hilbert, pode transformar esse problema não linear em um problema linearmente separável, demonstrando sua capacidade de lidar com não linearidades de forma eficiente.

---

### Embedding de dados clássicos

#### Conceito de encoding

O processo de codificação transforma dados clássicos em dados quânticos, necessário porque um computador quântico opera manipulando propriedades desses estados quânticos.

Essa codificação consiste na aplicação de um conjunto de portas quânticas sobre um estado inicial de qubits (geralmente inicializados como |0⟩^{⊗ n}) para mapear o vetor de características clássico x em um estado quântico |ψ(x)⟩:

$$
|\psi(x)\rangle = U_{encoding}(x)|0\rangle^{\otimes n}
$$

Onde U_{encoding}(x) é o circuito quântico de codificação (Quantum Feature Map) dependente dos dados clássicos.

A escolha desse método de codificação é importante porque afeta:
1. A capacidade do circuito de processar os dados (se o modelo conseguirá aprender as relações).
2. A profundidade e o número de portas do circuito — crítico no contexto atual, em que dispositivos quânticos são ruidosos.

#### Angle Embedding

Um dos métodos mais usados para dados vetoriais, que são comuns em aprendizado de máquina clássico.

Objetivo:
- Transpor um vetor de características clássico para um estado quântico mapeando cada feature para um ângulo de rotação em um qubit correspondente.

Como funciona?
- Utilizam-se portas de rotação unitárias (R_x, R_y ou R_z) em cada qubit, onde o ângulo dessa rotação é uma função do valor da feature (x_i):

$$
\text{Exemplo com } R_Y: \quad |\psi(\vec{x})\rangle = \left(\bigotimes_{i=1}^n R_Y(f(x_i))\right) |0\rangle^{\otimes n}
$$

Aqui f(x_i) escala/transforma a feature clássica x_i no ângulo.

- Vantagem: eficiente em número de qubits — n qubits para n features — resultando em circuitos rasos, ideal para dispositivos quânticos atuais.
- Limitação: portas de rotação isoladas apenas codificam as features; para criar correlações entre qubits é necessário adicionar portas de emaranhamento (H, CNOT, ...), o que aumenta a complexidade do circuito.

#### Custo computacional

No caso do Angle Embedding, cada feature mapeia para um qubit e exige uma porta de rotação, levando a complexidade O(n) em número de qubits e portas — aceitável para o estado atual da tecnologia, mas impraticável para datasets muito grandes. Existem outros métodos que usam número de qubits logarítmico em relação às features, porém envolvem circuitos mais complexos e possivelmente mais ruidosos; trata-se de um trade-off entre recursos de hardware e profundidade do circuito.

---

### O que são modelos variacionais

Modelos Quânticos Variacionais (MQV) são circuitos quânticos parametrizados projetados para serem otimizados por um computador clássico.

Circuito quântico parametrizado:
- Sequência de portas cujas ações dependem de um vetor de parâmetros θ — o ansatz.
- O circuito mapeia um estado de entrada para um estado de saída, com a transformação determinada por θ:

$$
U(\theta)|\psi_{in}\rangle = |\psi_{out}\rangle
$$

A estrutura típica do ansatz repete duas partes (as camadas):

- Portas de rotação (parte variacional): aplicam rotações dependentes de parâmetros (R_x(θ), R_y(θ), R_z(θ)) — atuam sobre qubits individuais.
- Portas de emaranhamento (parte de conectividade): criam emaranhamento entre qubits (CNOT, CZ, CSWAP) — sem parâmetros.

A capacidade do modelo cresce com o número de camadas repetidas.

#### O algoritmo híbrido

1. Embeddings de dados: mapeamento dos dados clássicos para um estado quântico inicial.
2. Ansatz: aplicação de U(θ) — o coração do aprendizado; parâmetros θ são ajustados.
3. Medição (expectation value): mede-se uma observável do estado de saída para obter um valor clássico.
4. Otimização clássica: o valor de expectativa alimenta uma loss function; um otimizador clássico (ex.: Adam) atualiza θ para minimizar a função de custo.

Analogia ao clássico:
- O ansatz corresponde à arquitetura de uma rede neural (semelhante a template de camadas), enquanto os parâmetros θ são análogos a pesos e biases. A diferença é que, no quântico, o ajuste usa gradientes quânticos (ex.: parameter-shift rule) em vez de backpropagation.

Implicação para QML:
- A hibridização (clássico + quântico) é essencial para aproveitar o poder da computação quântica atual, ainda ruidosa. O ansatz é versátil e pode ser aplicado a problemas de classificação, otimização, etc.

---

### Parâmetros ajustáveis (θ) nos circuitos parametrizados

Os parâmetros θ definem as rotações nas portas do circuito U(θ) e controlam o comportamento e a capacidade de representação do modelo quântico.

Como são atualizados durante o treinamento?
1. Cálculo da função de custo C(θ): o circuito é executado com θ e a medição fornece a métrica de desempenho.

$$
C(\theta) = \langle \psi(\theta) | H | \psi(\theta) \rangle
$$

2. Cálculo do gradiente: estima-se ∂C/∂θ_i para orientar as atualizações.
3. Atualização dos parâmetros: um otimizador clássico (ex.: Adam) usa o gradiente para atualizar θ.

Esses passos são repetidos até a convergência da função de custo.

Analogia aos pesos clássicos:
- Pesos e vieses controlam conexões entre neurônios; parâmetros quânticos controlam as rotações dos estados quânticos. Ambos visam ajustar a transformação dos dados.

---

### Quantum Gradients: Parameter-Shift Rule

Gradientes quânticos são derivadas parciais do valor de expectativa da função de custo em relação aos parâmetros θ.

Diferenças em relação ao gradiente clássico:
- Backpropagation exige acesso a estados intermediários e armazenamento de parâmetros, o que não é viável nos dispositivos quânticos atuais.
- Em quântica, métodos como a Parameter-Shift Rule permitem obter derivadas por avaliações de expectativa em pontos deslocados do parâmetro, sem acessar estados internos.

Parameter-Shift Rule:
- A derivada é obtida executando a função de custo várias vezes com deslocamentos fixos nos parâmetros (avaliando em θ + s e θ − s) e combinando as expectativas para calcular a derivada exata.

SPSA (Simultaneous Perturbation Stochastic Approximation):
- Otimizador estocástico que estima gradientes perturbando todos os parâmetros simultaneamente em direções aleatórias.
- Mais rápido em termos de número de avaliações, porém gera estimativas ruidosas do gradiente; tende a convergir ao longo do tempo.

Importância para MQV:
- Gradientes quânticos habilitam a otimização dos parâmetros e são compatíveis com hardware quântico existente, permitindo cálculos de derivadas usando apenas operações unitárias e medições.
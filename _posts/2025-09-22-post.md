---
layout: post
title: "Classificador Bayesiano"
subtitle: Teorema de Bayes na IA
date: 2025-09-22
author: Gabriel A. Belo
categories: [Computação]
tags: [IA | Machine Learning]
---

Hoje trago o primeiro tópico relacionado à IA, especificamente ao campo de **Machine Learning**, aprofundando em um modelo de decisão simples, mas muito eficaz, conhecido como **Classificador Bayesiano**.

Vamos começar explorando sua base teórica, que, nesta situação, se alinha diretamente com o **algoritmo de machine learning**. Veremos assim probabilidades condicionais, o teorema de Bayes e então sua aplicação.

### Sumário

- [Probabilidade Condicionada](#probabilidade-condicionada)
- [Teorema de Bayes](#teorema-de-bayes)
- [Classificador Bayesiano](#classificador-bayesiano)

### Probabilidade Condicionada

A base para o entendimento do **Teorema de Bayes** começa no conceito de probabilidade condicional:

$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$

Essa fórmula refere-se à probabilidade de um evento **A** ocorrer, sabendo que outro evento **B** já ocorreu.

- $$P(A|B)$$ 
é a probabilidade condicional de A dado B
- $$P(A \cap B)$$ é a probabilidade da interseção de A e B (ambos os eventos ocorrerem)
- $$P(B)$$ é a probabilidade do evento B, sendo essa probabilidade maior que 0

**Definição conceitual**: Sejam A e B dois eventos associados ao experimento $$\beta$$. Denotaremos por probabilidade condicional de A dado B, a probabilidade do evento A, quando B tiver ocorrido.

A depender dos dados que temos disponíveis, também temos dois teoremas complementares:

- **Teorema da multiplicação de probabilidades**

$$P(A \cap B) = P(B|A)P(A)$$

$$P(A \cap B) = P(A|B)P(B)$$

> Calcula a interseção dos eventos, quando a condicional e o evento descondicionado são conhecidos.

- **Teorema da probabilidade total**
<div class="calculo-scroll">
$$P(A) = P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + ... + P(A|B_k)P(B_k)$$
</div>

- As partições $$B_1, B_2, ..., B_k$$ do espaço amostral são definidas quando:

$$B_i \cap B_j = \emptyset$$

$$\cup_{i=1}^{k} B_i = S$$

$$P(B_i) > 0 \text{ para todo i}$$

- Em resumo, quando o experimento $$\beta$$ é realizado, um, e somente um, dos eventos $$B_i$$ ocorre.

- Dessa forma, podemos definir que se A é um evento referente ao espaço amostral S:

<div class="calculo-scroll">
$$A = (A \cap B_1) \cup (A \cap B_2) \cup \text{ ... } \cup (A \cap B_k)$$
</div>

- E como todos os eventos $$A \cap B_i$$ são mutuamente excludentes, podemos aplicar a **propriedade da adição de eventos mutuamente excludentes**:

<div class="calculo-scroll">
$$P(A) = P(A \cap B_1) + P(A \cap B_2) + ... + P(A \cap B_k)$$
</div>

> Calcula o evento descondicionado, quando a condicional e as partições do espaço amostral são conhecidas.

### Teorema de Bayes

Esse teorema permite que calculemos probabilidades de um evento particular $$B_i$$, dado que o evento A tenha ocorrido.

<div class="calculo-scroll">
$$P(B_i|A) = \frac{P(A|B_i)P(B_i)}{\sum_{j=1}^{k} P(A|B_j)P(B_j)} i = 1, 2,..., k$$
</div>

Enquanto a fórmula da **probabilidade condicional** nos permite calcular a probabilidade de A dado que B, o **Teorema de Bayes** oferece uma alternativa de calcular essa probabilidade usando a probabilidade de B dado que A, e as probabilidades absolutas de cada evento.

Por exemplo:

- É fácil saber a probabilidade de um teste dar positivo (A) dado que a pessoa tem a doença (B). Isso é a precisão do teste.

- O que o paciente realmente quer saber é a probabilidade de ter a doença (B) dado que o teste deu positivo<br> (A = é positivo).

**Demonstração matemática**:

- A probabilidade condicional que queremos descobrir, $$B_i \text{ dado que A é } \frac{P(A \cap B_i)}{P(A)}$$
- A probabilidade condicional que já sabemos, A dado que $$B_i \text{ é } \frac{P(A \cap B_i)}{P(B_i)}$$

- A equação anterior, isolando a probabilidade da interseção, ficaria 

$$P(A \cap B_i) = P(B_i | A) \cdot P(A)$$

- E no denominador temos que 

<div class="calculo-scroll">
$$P(A) = P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + ... + P(A|B_k)P(B_k)$$
</div>

Por fim, resumimos tudo isso na equação inicial.

**Eventos dependentes**: 

Para que o teorema se mostre eficiente, os eventos precisam ter algum tipo de dependência entre si:

Em casos que chamamos de **dependências extremas**, saber que B já ocorreu define a probabilidade de ocorrência de A.

$$A \cap B = \emptyset$$

- Eventos mutuamente excludentes
- A probabilidade de A dado que B é igual a 0
- A ocorrência de B impede a ocorrência de A

$$A \supset B$$

- A probabilidade de B dado que A é igual a 1

O ideal para aplicação do teorema são casos em que a probabilidade está entre 0 e 1.

> A e B serão eventos independentes se, e somente se, $$P(A \cap B) = P(A)P(B)$$

### Classificador Bayesiano

O classificador bayesiano se divide em dois modelos: o modelo ideal (uma referência teórica) e o ingênuo.

- **Modelo Ideal**

Consegue usar todos os dados probabilísticos disponíveis para calcular a probabilidade de cada classe $$\omega_i$$ levando em conta as probabilidades conjuntas de todas as suas características. Ou seja, ele considera como as características se influenciam mutuamente.

Porém, para conseguir calcular todas essas probabilidades conjuntas, é preciso um conjunto massivo de dados, e isso torna o modelo quase impossível, por não ter a quantidade de informações necessárias para estabelecer essas relações.

- **Modelo Ingênuo**

Esse nome é dado pois seu método consiste em supor que todas as características analisadas são **independentes entre si**, dado que pertencem a uma determinada classe.

Essa suposição simplifica drasticamente o cálculo. Em vez de calcular a probabilidade conjunta de todas as características juntas, ele apenas multiplica as probabilidades individuais de cada característica.

Por exemplo: "Qual a probabilidade de a palavra 'oferta' aparecer 10 vezes, dado que a mensagem é spam?".

Em síntese, dada uma classe $$\omega_1$$ = "Spam" e uma classe $$\omega_2$$ = "Não é spam", analisamos as características da mensagem recebida, dado que ela pertença à classe "spam".

Após multiplicarmos todas essas probabilidades, temos a probabilidade dessa mensagem ser da classe "spam", depois repetimos o processo para a classe "não é spam", e o maior resultado define a classe da mensagem.

$$P(\omega_i|x) \propto P(\omega_i) \cdot \prod_{j=1}^{n} P(x_j|\omega_i)$$

> Dizemos que o resultado dessa condicional é proporcional a esse modelo ingênuo, conferindo uma exatidão bem similar com um cálculo muito mais simples e rápido.

- **Aplicação no Machine Learning**

Por fim, a aplicação desse modelo consiste, inicialmente, em utilizar um conjunto de dados já classificados corretamente. A máquina utiliza o modelo de Bayes para identificar todas as probabilidades necessárias para obter os resultados dessas classificações.

Depois, são fornecidos novos dados sem classificação e, com base nas probabilidades registradas, o classificador utiliza novamente o modelo para classificar esse dado, aplicando as probabilidades relacionadas às características dos dados e resultando em uma classificação.

Assim, com uma boa quantidade de dados, para tarefas simples como definir se uma mensagem é ou não spam, o **classificador bayesiano ingênuo** pode apresentar uma acurácia surpreendente para sua baixa complexidade.

> A base de dados, nesse caso, é muito importante, visto que novas mensagens com características nunca antes vistas na etapa de treinamento podem gerar resultados inadequados, embora existam técnicas de minimização, como a **Laplace Smoothing**.
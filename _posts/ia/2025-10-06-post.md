---
layout: post
title: "Outros Paradigmas de ML"
subtitle: Semi-supervisionado, auto-supervisionado e por reforço
date: 2025-10-06
author: Gabriel A. Belo
categories: [Computação]
tags: [IA | Machine Learning]
---

No post anterior sobre *Machine Learning*, vimos os dois paradigmas fundamentais: o aprendizado supervisionado e o não supervisionado.

Porém, ao longo dos anos, houve evoluções nessas abordagens que deram origem a novas, buscando solucionar os desafios do mundo real.

Atualmente, existem inúmeras variações que misturam esses paradigmas a fim de se adequar a problemas cada vez mais específicos. 

Neste post, vamos finalizar esse tópico abordando três deles: **Aprendizado Semi-Supervisionado**, **Aprendizado Auto-Supervisionado** e **Aprendizado por Reforço**.

### Sumário

- [Aprendizado Semi-Supervisionado](#aprendizado-semi-supervisionado-ssl)
- [Aprendizado Auto-Supervisionado](#aprendizado-auto-supervisionado-self-sl)
- [Aprendizado por Reforço](#aprendizado-por-reforço-rl)
- [Aplicações](#aplicações)

### Aprendizado Semi-Supervisionado (SSL)

Como comentei na parte de aprendizado não supervisionado, embora seu desempenho seja inferior ao supervisionado, seu uso é muito difundido pela falta de dados rotulados.

Rotular grandes volumes de dados é caro, demorado e exige uma ação humana. Nessa perspectiva, o aprendizado semi-supervisionado surge como uma solução.

Interpretável como um "meio-termo" entre as duas abordagens anteriores, ele é uma técnica que utiliza um pequeno conjunto de dados rotulados e uma grande quantidade de dados não rotulados no seu treinamento.

O modelo cria uma base geral a partir dos dados não rotulados (com técnicas não supervisionadas) e depois refina e consolida sua base a partir dos dados rotulados (supervisionado).

Na prática, a principal técnica de **Pseudo-Labeling** inicialmente treina o modelo com os dados rotulados, e após esse treinamento ele constrói *pseudo-rótulos* para o conjunto de dados não rotulados.

Após essa etapa, o modelo é treinado novamente com os dados rotulados e agora com os pseudo-rotulados, e com esse ciclo iterativo os poucos rótulos ganham um "peso" maior no treinamento supervisionado, pela vasta quantidade de dados pseudo-rotulados.

### Aprendizado Auto-Supervisionado (Self-SL)

Esse paradigma compartilha com o anterior uma solução para a escassez de dados rotulados. No entanto, existe uma diferença na função dos dados não rotulados e na origem dos rótulos. 

Nessa abordagem, o modelo é capaz de gerar automaticamente seus próprios rótulos para aprender, a partir de dados de entrada não rotulados.

Não é preciso de rótulos humanos na fase inicial; é criada uma **tarefa-pretexto** (*pretext task*), como "prever a próxima palavra em uma frase", que pode ser resolvida pelo próprio dado.

Ao completar essa tarefa, o modelo extrai informações que foram úteis para a resolução dessa tarefa e agrega esse conhecimento à sua base de dados.

Assim, em uma tarefa real, esse conhecimento é transferido (*downstream-task*) e sofre um ajuste a partir de um pequeno conjunto de dados rotulados (*fine-tuning*), para atender ao objetivo final.

### Aprendizado por Reforço (RL)

Esse é o paradigma mais diferenciado até então, onde, ao invés de dados estáticos, o objetivo é a tomada de decisão sequencial em um ambiente dinâmico.

Nesse caso, o agente é ensinado a agir de forma autônoma, através de um aprendizado quase humano, por **tentativa e erro**.

Na prática, o modelo oferece inúmeras formas de agir dentro de um ambiente dinâmico, com o único objetivo do agente sendo maximizar uma **recompensa acumulativa**.

Esse objetivo guia as atitudes do agente, que começa a aprender padrões que levam ao ganho dessa recompensa e a evitar o que produz um retorno negativo (punições).

Não existe um gabarito, como nos dados estáticos; o agente recebe apenas o feedback se houve uma recompensa positiva ou uma punição. 

Assim, são realizadas inferências de quais ações retornam um maior valor, fixando como atitudes melhores para aquela situação.

Isso ocorre em um ciclo iterativo, visto que, por se tratar de um ambiente dinâmico, a cada ação realizada pelo agente é preciso reavaliar as possibilidades.

> É um processo de exploração (ambiente) e aproveitamento (buscar máxima recompensa).

### Aplicações

Para finalizar o entendimento básico sobre esses três paradigmas, vou apresentar algumas aplicações práticas:

- **SSL**: É valioso sempre que o volume de dados é alto, mas sua rotulagem é cara, demorada ou exige um conhecimento especializado.

    - **Detecção de Fraudes em Finanças**: Para transações bancárias, há enormes quantidades de fraudes e poucas são identificadas diretamente por uma equipe de segurança (dados rotulados). Assim, é utilizada a distribuição das transações normais para identificar possíveis fraudes e direcionar com eficiência a equipe de segurança.

- **Self-SL**: Atualmente é o motor por trás dos avanços do *Deep Learning*, permitindo que modelos extraiam conhecimento de dados brutos da internet.

    - **Grandes Modelos de Linguagem** (*LLMs*): Esses modelos são pré-treinados para resolver *pretext tasks*, como a previsão de palavras faltantes em inúmeros textos. Dessa forma, eles conseguem adquirir uma compreensão profunda em áreas de gramática, contexto e conhecimentos gerais, que podem ser adaptados para inúmeras tarefas.

- **RL**: Muito utilizado para o avanço da robótica atual, é ideal para problemas que envolvem uma sequência de decisões, que possuem um objetivo a longo prazo.

    - **Carros Autônomos**: Nesse caso, o agente é o sistema de direção, que está na rua (ambiente) e possui como objetivo chegar a um destino. Assim, são tomadas inúmeras decisões sequenciais (acelerar, frear, virar) que maximizam a recompensa (chegar ao destino com segurança e eficiência) e minimizam acidentes (punição).
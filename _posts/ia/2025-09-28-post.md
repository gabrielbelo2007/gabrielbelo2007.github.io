---
layout: post
title: "Aprendizado Supervisionado e Não Supervisionado"
subtitle: Os pilares do Machine Learning
date: 2025-09-28
author: Gabriel A. Belo
categories: [IA]
tags: [Machine Learning]
---

No último post sobre IA, foi apresentado um algoritmo de **Machine Learning** (ML), o **Teorema de Bayes**. Entretanto, algoritmos são apenas uma das aplicações práticas do ML.

Por isso, neste post vamos dar alguns passos para trás, para construir uma base sólida e entender o que exatamente é o **Aprendizado de Máquina**.

Explorando seus paradigmas fundamentais: o **Aprendizado Supervisionado** e o **Aprendizado Não Supervisionado**.

### Sumário

- [O que é *Machine Learning*?](#o-que-é-machine-learning)
- [Aprendizagem Supervisionada](#aprendizagem-supervisionada)
- [Tarefas Supervisionadas](#tarefas-supervisionadas)
- [Aprendizagem Não Supervisionada](#aprendizagem-não-supervisionada)
- [Tarefas Não Supervisionadas](#tarefas-não-supervisionadas)
- [*Overfitting* e *Underfitting*](#bônus-overfitting-e-underfitting)

### O que é *Machine Learning*?

A aprendizagem de máquina é um enorme campo dentro da **Inteligência Artificial**, que se dedica a construir métodos que permitam que as máquinas "aprendam" a partir de grandes bancos de dados.

Assim, essas máquinas tornam-se capazes de tomar decisões ou fazer previsões por meio de padrões estruturados pela análise desses dados.

Em síntese, entregamos dados e escolhemos um método para que a máquina os manipule e forneça uma resposta útil, sem que os desenvolvedores precisem pensar em todas as variáveis possíveis e construir um algoritmo restrito sobre como a máquina deve agir.

Portanto, se o objetivo é fazer a máquina aprender a partir de dados, como nós humanos, existem variados "métodos de estudo" *(paradigmas)* que definem nosso "aprendizado" *(aplicação)*. As máquinas não são diferentes.

No ML, existem três paradigmas principais: **Supervisionado**, **Não Supervisionado** e **Por Reforço**. Neste post, vamos focar no *Supervised Learning (SL)*, que é o mais comum para aplicações no dia a dia.

### Aprendizagem Supervisionada

Ainda fazendo uma conexão com a programação tradicional, imagine a seguinte situação:

- Eu quero fazer uma criança aprender a diferença entre gato e cachorro. Para isso, tenho duas opções:

    - Entregar um "manual" contendo todas as regras para distinguir um gato de um cachorro, diferenças na pata, orelhas, boca...
    - Mostrar diversos exemplos de gatos e cachorros e explicitar qual é qual.

Logicamente, a segunda opção parece melhor, até porque nós, que já sabemos diferenciar um gato de um cachorro, talvez nem saibamos exatamente tudo que analisamos para fazer essa distinção.

Isso se tornou algo trivial em algum momento, a partir de uma repetida visualização de ambas as espécies.

> Esse paradigma é ideal para quando sabemos a resposta certa, mas não conseguimos formular uma regra que retorne essa resposta.

Como vimos, para problemas onde a quantidade de variáveis a serem analisadas é infinita, utilizamos o ML. 

Nesse caso, com o **aprendizado supervisionado**, fornecemos uma grande quantidade de dados rotulados (isso é um gato ou isso é um cachorro), que chamamos de **supervisão**.

Essa supervisão é importante, pois, sabendo a resposta, a máquina pode avaliar sua previsão, aprendendo as características que definem um cachorro e um gato, e adequando seu modelo.

Ou seja, o algoritmo "aprende" a construir uma função (modelo de ML) onde uma nova foto gera uma previsão do rótulo, que é comparado com o rótulo real da imagem.

Em caso de erro (medido pela *loss function*), os parâmetros da função são ajustados e tenta-se novamente, até que o erro seja minimizado.

> Esses termos mais técnicos, como **Função de Perda** (*Loss Function*) ou minimização de erro, são a forma como a máquina quantifica o seu desempenho.

### Tarefas Supervisionadas

Existem dois tipos de problemas principais para os quais o **Aprendizado Supervisionado** é aplicado: **Classificação** (como o exemplo do gato/cachorro) e **Regressão**.

Como já vimos um exemplo de classificação no tópico anterior, vamos entender como exatamente definimos esse tipo de problema.

#### Classificação

A **Classificação** é usada para prever uma **categoria** ou **classe**, sendo assim, ao invés de estimativas numéricas, o objetivo é responder à pergunta: **"A qual grupo esse novo dado pertence?"**

No exemplo anterior, tínhamos duas categorias (gato ou cachorro) e nosso modelo, ao receber uma nova imagem, precisava decidir a qual desses grupos a imagem pertencia. Ou seja, o trabalho do **algoritmo de classificação** é construir uma função que consiga separar as categorias de forma eficiente.

Os dois formatos principais são a **classificação binária**, com apenas duas categorias possíveis (gato ou cachorro), ou a **classificação multiclasse**, quando há mais de duas categorias possíveis.

> A supervisão é essencial para que o algoritmo entenda como sua função deve separar os dados dentro das categorias, permitindo seu ajuste.

#### Regressão

Com a Classificação, respondíamos à pergunta "Qual é o grupo?", com a **Regressão** podemos responder à pergunta **"Qual é o valor?"** ou **"Quanto custa?"**, ou seja, fazemos previsões de valores numéricos.

Pense na situação em que queremos prever o preço de uma casa (o valor é contínuo). O **algoritmo de regressão** recebe como entrada diversas características referentes a essa casa, como tamanho, número de quartos, distância de locais movimentados...

Analisando diversas outras casas com características similares e seus valores de venda, o algoritmo cria um modelo que gera uma previsão do valor de venda da casa com os dados recebidos e, como fundamental desse paradigma, ele compara a sua previsão com o valor real de venda (rótulo) e ajusta seus parâmetros a fim de minimizar seus erros.

Nesse caso, diferentemente da Classificação, em que o erro está em separar em uma classe errada, o erro da **Regressão** está em gerar um valor numérico de saída muito distante do valor real.

> A função de perda da **Regressão** retorna uma distância numérica real entre o valor previsto e o valor real.

### Aprendizagem Não Supervisionada

No aprendizado supervisionado, o algoritmo recebia dados de entrada que possuíam rótulos (a resposta correta).

Assim, após a predição pelo algoritmo, era verificado se o resultado estava correto e isso permitia ao modelo se autoajustar ("aprender").

No **Aprendizado Não Supervisionado**, o algoritmo funciona como um explorador, sendo alimentado com dados não rotulados, ou seja, não existe uma regra definida para a "resposta correta".

A intenção é que o algoritmo descubra, por conta própria, padrões que consigam agrupar, conectar ou simplificar os dados em estruturas com características similares.  

É importante quando, dentre inúmeras variáveis, queremos descobrir quais delas tornam certos dados próximos ou revelar como é formada a estrutura de um conjunto de dados (análise exploratória).

> Esse paradigma é historicamente inferior ao anterior, porém a maior parte dos dados do mundo real não são rotulados, tornando seu uso viável.

### Tarefas Não Supervisionadas

Existem três grandes categorias de problemas para os quais o **Aprendizado Não Supervisionado** é aplicado:
**Clustering** (agrupamento), **Redução de Dimensionalidade** e **Regras de Associação**.

#### Clustering

Esse processo divide dados semelhantes em grupos (*clusters*), onde membros de um mesmo grupo têm uma semelhança notavelmente maior em relação a membros de outros grupos.

Por exemplo, uma loja pode utilizar essa técnica para dividir seus clientes em grupos, de acordo com padrões de compra, gênero, poder aquisitivo...(variáveis de entrada) 

O que permite direcionar a construção das campanhas de marketing com um público-alvo bem definido.

Perceba que não existe um rótulo, onde se um cliente é mulher, tem 40 anos e é da classe média então o grupo dela é X; a inferência entre essas características e o grupo ao qual o cliente pertence é esquematizada pelo próprio algoritmo.

#### Redução de Dimensionalidade

Nesse segundo tipo de problema, temos um conjunto de dados com uma variedade enorme de variáveis (dimensões), e tentar usar um processo de *clustering* em um conjunto como esse torna-se lento e ineficiente.

Por isso, aplicamos esse processo que busca simplificar o conjunto de dados, encontrando uma maneira de representar a maior parte da informação original, mas com uma quantidade bem reduzida de variáveis.

Na prática, o que acontece é que são analisadas as características que definem a posição de um dado nesse espaço de múltiplas dimensões; trata-se de uma visualização geométrica onde as variáveis são os eixos.

E então, após essa análise, são criadas novas variáveis sintéticas, a partir da combinação das variáveis originais, capturando os pontos mais importantes.

Assim, é mantida uma representação fiel à posição original dos dados, mas em um espaço mais compacto e fácil de administrar para aplicação de outras técnicas.

#### Regras de Associação

Por fim, nessa última técnica, o objetivo é procurar regras que descrevem relações e dependências frequentes entre variáveis (ou itens) de grandes bases de dados.

Pense em um supermercado que quer saber quais itens costumam ser comprados frequentemente juntos.

Assim, essa técnica é aplicada aos registros de compras dos clientes, retornando quais itens tendem a aparecer em coocorrência na mesma transação de compra.

Isso não produz uma regra definitiva, mas evidencia padrões que ajudam na otimização de layouts do supermercado, indicando quais itens devem ficar próximos entre si, a fim de instintivamente auxiliar o cliente e engajá-lo a comprar.

### Bônus: *Overfitting* e *Underfitting*

Com isso, abordamos as principais tarefas relacionadas aos pilares fundamentais do **Aprendizado de Máquina**.

Percebemos que a construção das funções envolve a busca constante pela minimização de erros; isso é o que move o aprendizado do modelo.

No entanto, se o modelo for ajustado em excesso ou de forma insuficiente, ele não será útil no mundo real. Isso nos leva a dois problemas cruciais na construção de modelos de *Machine Learning*: **Overfitting** e **Underfitting**.

No *Overfitting*, temos o "aluno que decora a apostila": o modelo, após um treinamento exaustivo com o mesmo conjunto de dados, torna-se muito complexo e começa a decorar os desvios de erro e os casos de exceção.

Assim, para dados desse conjunto, o modelo acerta quase 100%, porém, para dados nunca antes vistos, essa taxa de acerto despenca, pois a sua função não é realmente aplicável a qualquer contexto.

Já no *Underfitting*, a situação é contrária: "o aluno que não estudou o suficiente". O modelo foi treinado com poucos dados e não conseguiu captar nem padrões básicos desses dados, ou seja, ele não consegue uma taxa de acerto para os dados do conjunto de treinamento, nem para novos dados.

No fim, a meta ao construir esses modelos é encontrar um equilíbrio nesse treinamento, que permita uma constância alta para dados que pertencem ao conjunto de treinamento e para novos dados, mesmo que isso signifique não atingir 100% de acerto, mas chegar bem próximo disso.
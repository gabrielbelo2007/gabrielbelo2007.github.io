---
layout: post
title: "Introdução a Ciência da Computação"
subtitle: O que significa estudar computação?
date: 2025-09-08
author: Gabriel A. Belo
categories: [Programação]
tags: [CS50 | Ciência da Computação]
---

*"Ciência da Computação: é a área que estuda os fundamentos teóricos e práticos da computação, focando no desenvolvimento de softwares e sistemas, algoritmos, segurança da informação, inteligência artificial e redes de computadores para resolver problemas complexos."* 

Essa é a definição encontrada no Google, ao pesquisar "Ciência da Computação", contudo a parte mais importante dessa definição e que carrega realmente o foco dessa área de pesquisa, está na resolução de problemas. Afinal, todo o restante é produzido a fim de resolver ou aprimorar problemas existentes em nossas vidas, e nessa perspectiva podemos interpretar que os problemas a serem resolvidos são o que chamamos de *inputs* e através de linguagens e tecnologias desenvolvidas pelos cientistas da computação, obtemos soluções, os nossos *outputs*.

Então nesse post, vamos procurar entender sobre esse campo de estudos tão abrangente, que vem a vários anos ajudando a nossa sociedade a evoluir cada vez mais.

> O conteúdo aqui apresentado, tem forte influência do curso CC50 de Harvard, especialmente sua distribuição com legendas e anotações em português, da [Fundação Estudar.](https://ead.estudar.org.br/c/cc50-o-curso-de-ciencia-da-computacao-de-harvard-no-brasil/)

### Sumário

- [Representação de Dados](#representação-de-dados)
- [Algoritmos e Tempo de Execução](#algoritmos-e-tempo-de-execução)
- [Funções Básicas]()

### Representação de Dados

Nós humanos, utilizamos diferentes formas de contabilizar dados a depender do contexto, como os nossos próprios dedos, que compõe um sistema unário, onde cada dígito (nesse caso dedo) representa um único valor 1, sendo assim com as duas mãos só podemos contar até 10, visto que possuímos apenas 10 dedos.

Contudo, os computadores trabalham internamente com um sistema de contagem, denominado de **binário**, ou seja, sequências de 0s e 1s (conhecidos como *bits*), assim todo e qualquer dado precisa ser convertido para esse sistema, para que o computador consiga armazená-lo em sua memória. Em termos computacionais, temos que a eletricidade é a vida de todo o processamento dos computadores, sendo assim essa representação dos bits em 0s e 1s, pode ser representada como desligado e ligado, e a ordem na qual esses "interruptores" estão ligados e desligados, ilustra os diferentes tipos de valores.

<!-- Adicionar algum tipo de contexto a respeito do que são bytes -->

Dessa forma, precisamos de converter os diferentes tipos de dados que o computador pretende armazenar, para que ele consiga trabalhar com essas informações de forma padronizada. Vamos destrinchar cada uma dessas conversões, começando pelo tipo de dado númerico:

#### Números

O sistema decimal, utilizado cotidianamente nos nossos cálculos matemáticos, nos permite contar de 0 a 9 (10 dígitos por algarismo), onde a cada posição do algarismo na representação de um número, define um avanço em potências de base 10, ou seja, o número 100 representa 1 * 10^2 +  0 * 10^1 + 0 * 10^0 = 100, e assim é feito a representação de todos os demais números desse sistema, porém essa forma de pensar se torna automática para nós.

Quando se trata dos computadores, como dito anteriormente, eles utilizam um sistema binário, ou seja, ele conta apenas de 0 a 1, e a cada posição desses algarismos na representação de um número, o avanço ocorre em potências de base 2, ou seja, o número decimal 12, seria representado em binário como 1100, de forma que 1 * 2^3 + 1 * 2^4 + 0 * 2^1 + 0 * 2^0 = 12.

Essa conversão em termos matemáticos, pode ser realizada dividindo o número decimal sucessivamente por 2 e obtendo o resto da divisão do último para o primeiro, e então unindo esses restos nessa ordem, teremos a representação em binário de um número inteiro.

> Para números decimais, o processo é diferente, mas a ideia aqui é apenas entender como funciona o sistema binário.

#### Textos e Caracteres

Para a representação de textos, existem tabelas de codificação, que possuem um mapeamento de cada caractere para um número em binário, assim após a identificação do computador de que esse dado é textual, ele utiliza um dos sistemas de mapeamento para registrar cada caractere e interpretá-lo através dos seus bits correspondentes. Os sistemas mais utilizados atualmente são:

ASCII (American Standard Code for Information Interchange): Utiliza 1 byte (8 bits) por caractere, permitindo 256 símbolos diferentes. Por exemplo, a letra 'A' é representada pelo número 65, que em binário é 01000001.

UTF-8: Sistema mais moderno que pode usar de 1 a 4 bytes por caractere, sendo compatível com ASCII e capaz de representar qualquer caractere Unicode. É o padrão mais utilizado atualmente na web. (Seu grande diferencial está no tamanho que pode ser utilizado por caractere, permitindo o processamento de caracteres especiais como emojis)

#### Imagens

<!-- Revisar essa parte -->

As imagens digitais são compostas por pixels, onde cada pixel contém informações de cor em formato numérico. O sistema examina cada pixel individualmente, analisando valores que representam intensidade de cor e outras características.

Em imagens coloridas, utiliza-se frequentemente o sistema RGB (Red, Green, Blue), onde cada cor primária é representada por um valor numérico. Por exemplo, em imagens de 24 bits, cada canal de cor utiliza 8 bits, permitindo 256 níveis de intensidade por cor.

### Algoritmos e Tempo de Execução

**em produção...**
---